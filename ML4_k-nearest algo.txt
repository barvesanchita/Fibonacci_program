.import pandas as pd
import numpy as np
data = pd.read_csv("diabetes.csv")
data.head()
.data.isnull().any()
.data.describe().T
.# Columns with missing values represented by zeros
columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
# Make a deep copy of the data
data_copy = data.copy(deep=True)
# Replace zero values in specified columns with NaN to indicate missing values
data_copy[columns_with_zeros] = data_copy[columns_with_zeros].replace(0, np.nan)
# Check for missing values
data_copy.isnull().sum()
.#To fill these Nan values the data distribution needs to be understood
p = data.hist(figsize = (20,20))
.# Replace missing values by assigning the result back to each column
data_copy['Glucose'] = data_copy['Glucose'].fillna(data_copy['Glucose'].mean())
data_copy['BloodPressure'] = data_copy['BloodPressure'].fillna(data_copy['BloodPressure'].mean())
data_copy['SkinThickness'] = data_copy['SkinThickness'].fillna(data_copy['SkinThickness'].median())
data_copy['Insulin'] = data_copy['Insulin'].fillna(data_copy['Insulin'].median())
data_copy['BMI'] = data_copy['BMI'].fillna(data_copy['BMI'].median())
.p = data_copy.hist(figsize = (20,20))
.!pip install missingno
.import missingno as msno
p = msno.bar(data)
.p=data.Outcome.value_counts().plot(kind="bar")
.#The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. The number of non-diabetics is almost twice the number of diabetic patients
import seaborn as sns
p=sns.pairplot(data_copy, hue = 'Outcome')
.import matplotlib.pyplot as plt
plt.figure(figsize=(12,10)) # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(data.corr(), annot=True,cmap ='RdYlGn') # seaborn has very simple solution for heatmap
.plt.figure(figsize=(12,10)) # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(data_copy.corr(), annot=True,cmap ='RdYlGn') # seaborn has very simple solution for heatmap
.from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X = pd.DataFrame(sc_X.fit_transform(data_copy.drop(["Outcome"], axis=1)),
                 columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])
X.head()

.# Define the target variable
y = data_copy['Outcome']
# Split the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42, stratify=y)
# Import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
# Lists to store train and test scores
train_scores = []
test_scores = []
# Loop through different values of k (neighbors)
for i in range(1, 15):
    # Initialize the KNN classifier with i neighbors
    knn = KNeighborsClassifier(n_neighbors=i)
    # Fit the model on the training data
    knn.fit(X_train, y_train)
    # Append the training score to train_scores
    train_scores.append(knn.score(X_train, y_train))
    # Append the testing score to test_scores
    test_scores.append(knn.score(X_test, y_test))
# Calculate the maximum test score and the corresponding k values
max_test_score = max(test_scores)
test_score_index = [i for i, v in enumerate(test_scores) if v == max_test_score]
# Print the maximum test score and corresponding k values
print('Max test score: {:.2f}% and k = {}'.format(max_test_score * 100, list(map(lambda x: x + 1, test_score_index))))

.plt.figure(figsize=(12,5))
plt.figure(figsize=(12, 5))
p = sns.lineplot(x=range(1, 15), y=train_scores, marker='*', label='Train Score')
p = sns.lineplot(x=range(1, 15), y=test_scores, marker='o', label='Test Score')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Score')
plt.title('Train vs Test Scores for Different k values in KNN')
plt.legend()
plt.show()

.# K=11
#Setup a knn classifier with k neighbors
knn = KNeighborsClassifier(11)
knn.fit(X_train,y_train)
knn.score(X_test,y_test)

.!pip install mlxtend
.from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt

# Set the values for the filler feature values and ranges
value = 20000
width = 20000
# Pass the DataFrame `X` directly to plot_decision_regions
plot_decision_regions(X.values, y.values, clf = knn, legend =2,filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value},
filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width},
X_highlight=X_test.values)
# Add title and show the plot
plt.title("KNN with Diabetes Data")
plt.show()

.from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score
y_pred = knn.predict(X_test)
cnf_matrix = confusion_matrix(y_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

.from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score
import pandas as pd
# Define the function with proper indentation
def model_evaluation(y_test, y_pred, model_name):
    acc = accuracy_score(y_test, y_pred)  # Indented inside the function
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    f2 = fbeta_score(y_test, y_pred, beta=2.0)
    # Creating the results DataFrame
    results = pd.DataFrame([[model_name, acc, prec, rec, f1, f2]],
                           columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score", "F2 Score"])
    # Sorting the DataFrame by Precision, Recall, and F2 Score
    results = results.sort_values(["Precision", "Recall", "F2 Score"], ascending=False)
    return results
# Example usage of the function
results = model_evaluation(y_test, y_pred, "KNN")
print(results)

.# Alternate way
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

.from sklearn.metrics import auc, roc_auc_score, roc_curve
y_pred_proba = knn.predict_proba(X_test)[:,-1]
fpr, tpr, threshold = roc_curve(y_test, y_pred_proba)
classifier_roc_auc = roc_auc_score(y_test, y_pred_proba)
plt.plot([0,1],[0,1], label = "---")
plt.plot(fpr, tpr, label ='KNN (area = %0.2f)' % classifier_roc_auc)
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('Knn(n_neighbors=11) ROC curve')
plt.legend(loc="lower right", fontsize = "medium")
plt.xticks(rotation=0, horizontalalignment="center")
plt.yticks(rotation=0, horizontalalignment="right")